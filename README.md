# What is this?

This is my python repo in which I do silly things with local LLMs. 

You can reproduce it by getting llama.cpp and downloading some GGUF model.